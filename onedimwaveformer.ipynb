{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "def get_data(x_range, y_range, x_num, y_num):\n",
    "    x = np.linspace(x_range[0], x_range[1], x_num)\n",
    "    t = np.linspace(y_range[0], y_range[1], y_num)\n",
    "\n",
    "    x_mesh, t_mesh = np.meshgrid(x,t)\n",
    "    data = np.concatenate((np.expand_dims(x_mesh, -1), np.expand_dims(t_mesh, -1)), axis=-1)\n",
    "    \n",
    "    b_left = data[0,:,:] \n",
    "    b_right = data[-1,:,:]\n",
    "    b_upper = data[:,-1,:]\n",
    "    b_lower = data[:,0,:]\n",
    "    res = data.reshape(-1,2)\n",
    "\n",
    "    return res, b_left, b_right, b_upper, b_lower\n",
    "\n",
    "\n",
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "\n",
    "def make_time_sequence(src, num_step=5, step=1e-4):\n",
    "    dim = num_step\n",
    "    src = np.repeat(np.expand_dims(src, axis=1), dim, axis=1)  # (N, L, 2)\n",
    "    for i in range(num_step):\n",
    "        src[:,i,-1] += step*i\n",
    "    return src\n",
    "\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of PINNsformer\n",
    "# paper: PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks\n",
    "# link: https://arxiv.org/abs/2307.11833\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pdb\n",
    "\n",
    "\n",
    "class WaveAct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WaveAct, self).__init__() \n",
    "        self.w1 = nn.Parameter(torch.ones(1), requires_grad=True)\n",
    "        self.w2 = nn.Parameter(torch.ones(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w1 * torch.sin(x)+ self.w2 * torch.cos(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=256):\n",
    "        super(FeedForward, self).__init__() \n",
    "        self.linear = nn.Sequential(*[\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            WaveAct(),\n",
    "            nn.Linear(d_ff, d_ff),\n",
    "            WaveAct(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.act1 = WaveAct()\n",
    "        self.act2 = WaveAct()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x2 = self.act1(x)\n",
    "        # pdb.set_trace()\n",
    "        x = x + self.attn(x2,x2,x2)[0]\n",
    "        x2 = self.act2(x)\n",
    "        x = x + self.ff(x2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.act1 = WaveAct()\n",
    "        self.act2 = WaveAct()\n",
    "\n",
    "    def forward(self, x, e_outputs): \n",
    "        x2 = self.act1(x)\n",
    "        x = x + self.attn(x2, e_outputs, e_outputs)[0]\n",
    "        x2 = self.act2(x)\n",
    "        x = x + self.ff(x2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, N, heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.act = WaveAct()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x)\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, N, heads):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.act = WaveAct()\n",
    "        \n",
    "    def forward(self, x, e_outputs):\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs)\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "\n",
    "class PINNsformer(nn.Module):\n",
    "    def __init__(self, d_out, d_model, d_hidden, N, heads):\n",
    "        super(PINNsformer, self).__init__()\n",
    "\n",
    "        self.linear_emb = nn.Linear(2, d_model)\n",
    "\n",
    "        self.encoder = Encoder(d_model, N, heads)\n",
    "        self.decoder = Decoder(d_model, N, heads)\n",
    "        self.linear_out = nn.Sequential(*[\n",
    "            nn.Linear(d_model, d_hidden),\n",
    "            WaveAct(),\n",
    "            nn.Linear(d_hidden, d_hidden),\n",
    "            WaveAct(),\n",
    "            nn.Linear(d_hidden, d_out)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        src = torch.cat((x,t), dim=-1)\n",
    "        src = self.linear_emb(src)\n",
    "\n",
    "        e_outputs = self.encoder(src)\n",
    "        d_output = self.decoder(src, e_outputs)\n",
    "        output = self.linear_out(d_output)\n",
    "        # pdb.set_trace()\n",
    "        # raise Exception('stop')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINNsformer(\n",
      "  (linear_emb): Linear(in_features=2, out_features=32, bias=True)\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear): Sequential(\n",
      "            (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "            (1): WaveAct()\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): WaveAct()\n",
      "            (4): Linear(in_features=256, out_features=32, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (act1): WaveAct()\n",
      "        (act2): WaveAct()\n",
      "      )\n",
      "    )\n",
      "    (act): WaveAct()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear): Sequential(\n",
      "            (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "            (1): WaveAct()\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): WaveAct()\n",
      "            (4): Linear(in_features=256, out_features=32, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (act1): WaveAct()\n",
      "        (act2): WaveAct()\n",
      "      )\n",
      "    )\n",
      "    (act): WaveAct()\n",
      "  )\n",
      "  (linear_out): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
      "    (1): WaveAct()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): WaveAct()\n",
      "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "453561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/hzntchlx2j34g96rbgzm14q00000gn/T/ipykernel_22749/922921971.py:42: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.optim import LBFGS, Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "step_size = 1e-4\n",
    "# Train PINNsformer\n",
    "res, b_left, b_right, b_upper, b_lower = get_data([0,1], [0,1], 51, 51)\n",
    "res_test, _, _, _, _ = get_data([0,1], [0,1], 101, 101)\n",
    "\n",
    "res = make_time_sequence(res, num_step=5, step=step_size)\n",
    "b_left = make_time_sequence(b_left, num_step=5, step=step_size)\n",
    "b_right = make_time_sequence(b_right, num_step=5, step=step_size)\n",
    "b_upper = make_time_sequence(b_upper, num_step=5, step=step_size)\n",
    "b_lower = make_time_sequence(b_lower, num_step=5, step=step_size)\n",
    "\n",
    "res = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(device)\n",
    "b_left = torch.tensor(b_left, dtype=torch.float32, requires_grad=True).to(device)\n",
    "b_right = torch.tensor(b_right, dtype=torch.float32, requires_grad=True).to(device)\n",
    "b_upper = torch.tensor(b_upper, dtype=torch.float32, requires_grad=True).to(device)\n",
    "b_lower = torch.tensor(b_lower, dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "x_res, t_res = res[:,:,0:1], res[:,:,1:2]\n",
    "x_left, t_left = b_left[:,:,0:1], b_left[:,:,1:2]\n",
    "x_right, t_right = b_right[:,:,0:1], b_right[:,:,1:2]\n",
    "x_upper, t_upper = b_upper[:,:,0:1], b_upper[:,:,1:2]\n",
    "x_lower, t_lower = b_lower[:,:,0:1], b_lower[:,:,1:2]\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "model = PINNsformer(d_out=1, d_hidden=512, d_model=32, N=1, heads=2).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "optim = LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n",
    "\n",
    "print(model)\n",
    "print(get_n_params(model))\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 18/1000 [35:32<83:38:02, 306.60s/it]"
     ]
    }
   ],
   "source": [
    "loss_track = []\n",
    "\n",
    "pi = torch.tensor(np.pi, dtype=torch.float32, requires_grad=False).to(device)\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    def closure():\n",
    "        pred_res = model(x_res, t_res)\n",
    "        pred_left = model(x_left, t_left)\n",
    "        pred_right = model(x_right, t_right)\n",
    "        pred_upper = model(x_upper, t_upper)\n",
    "        pred_lower = model(x_lower, t_lower)\n",
    "\n",
    "        u_x = torch.autograd.grad(pred_res, x_res, grad_outputs=torch.ones_like(pred_res), retain_graph=True, create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x_res, grad_outputs=torch.ones_like(pred_res), retain_graph=True, create_graph=True)[0]\n",
    "        u_t = torch.autograd.grad(pred_res, t_res, grad_outputs=torch.ones_like(pred_res), retain_graph=True, create_graph=True)[0]\n",
    "        u_tt = torch.autograd.grad(u_t, t_res, grad_outputs=torch.ones_like(pred_res), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        loss_res = torch.mean((u_tt - 4 * u_xx) ** 2)\n",
    "        loss_bc = torch.mean((pred_upper) ** 2) + torch.mean((pred_lower) ** 2)\n",
    "\n",
    "        ui_t = torch.autograd.grad(pred_left, t_left, grad_outputs=torch.ones_like(pred_left), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        loss_ic_1 = torch.mean((pred_left[:,0] - torch.sin(pi*x_left[:,0])) ** 2)\n",
    "        loss_ic_2 = torch.mean((ui_t)**2)\n",
    "\n",
    "        loss_ic = loss_ic_1 + loss_ic_2\n",
    "\n",
    "        loss_track.append([loss_res.item(), loss_ic.item(), loss_bc.item()])\n",
    "        \n",
    "        loss = loss_res + loss_ic + loss_bc\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optim.step(closure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
